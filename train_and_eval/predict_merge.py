import os
import sys
import time
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import argparse
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from PIL import Image
import yaml

# === æ¨¡å‹å¯¼å…¥ ===
from models.UNet3D.unet3d import UNet3D
from models.UNet3D.unet3df import UNet3D_CSCL
from models.CropTypeMapping.models import FCN_CRNN
from models.BiConvRNN.biconv_rnn import BiRNNSequentialEncoder
from models.TSViT.TSViTdense import TSViT
from data.PASTIS24.data_transforms import Normalize
from data import get_dataloaders
import torchprofile  # ç”¨äºè®¡ç®—FLOPs


def get_model(config, device):
    model_config = config['MODEL']
    if model_config['architecture'] == "UNET3Df":
        return UNet3D_CSCL(model_config).to(device)
    if model_config['architecture'] == "UNET3D":
        return UNet3D(model_config).to(device)
    if model_config['architecture'] == "UNET2D-CLSTM":
        return FCN_CRNN(model_config).cuda()
    if model_config['architecture'] == "ConvBiRNN":
        return BiRNNSequentialEncoder(model_config, device).to(device)
    if model_config['architecture'] == "TSViT":
        return TSViT(model_config).to(device)
    raise NameError(f"Model architecture '{model_config['architecture']}' not supported.")


def read_yaml(yaml_path):
    if not os.path.exists(yaml_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {yaml_path}")
    with open(yaml_path, 'r', encoding='utf-8') as f:
        try:
            config = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise RuntimeError(f"YAML è§£æå¤±è´¥: {e}")
    return config


def get_device(device_ids, allow_cpu=True):
    if torch.cuda.is_available():
        return torch.device(f"cuda:{device_ids[0]}")
    elif allow_cpu:
        return torch.device("cpu")
    else:
        raise EnvironmentError("æ²¡æœ‰å¯ç”¨ GPU ä¸”æœªå¯ç”¨ CPUã€‚")


def load_from_checkpoint(model, checkpoint_path, device):
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ¨¡å‹æƒé‡æœªæ‰¾åˆ°: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint)
    print(f"âœ… æˆåŠŸåŠ è½½æƒé‡: {checkpoint_path}")


def apply_color_palette(img):
    # 4 ç±»é¢œè‰²ï¼šé»‘ã€çº¢ã€ç»¿ã€è“
    palette = [
                  0, 0, 0,  # class 0 - black
                  255, 0, 0,  # class 1 - red
                  0, 255, 0,  # class 2 - green
                  0, 0, 255  # class 3 - blue
              ] + [0] * (256 * 3 - 12)  # å¡«å……å‰©ä½™
    img.putpalette(palette)
    return img


def compute_flops(model, input_tensor):
    # ä½¿ç”¨ torchprofile æ¥è®¡ç®— FLOPs
    flops = torchprofile.profile_macs(model, input_tensor)
    return flops


def test_and_save_predictions(net, dataloader, config, device, save_dir="test_predictions"):
    os.makedirs(save_dir, exist_ok=True)
    net.eval()

    # æ·»åŠ æ¨ç†æ—¶é—´æµ‹é‡
    total_inference_time = 0.0
    total_samples = 0

    with torch.no_grad():
        # åŠ¨æ€è·å–è¾“å…¥å½¢çŠ¶
        print("ğŸ“ è·å–è¾“å…¥å½¢çŠ¶...")
        first_batch = next(iter(dataloader))
        inputs = first_batch['inputs'].to(device)
        input_shape = inputs.shape

        print(f"æ£€æµ‹åˆ°è¾“å…¥å½¢çŠ¶: {input_shape}")

        # ä½¿ç”¨å®é™…è¾“å…¥å½¢çŠ¶è¿›è¡Œé¢„çƒ­
        print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
        dummy_input = torch.randn_like(inputs)  # ä½¿ç”¨ç›¸åŒçš„å½¢çŠ¶
        for _ in range(5):
            _ = net(dummy_input)

        # è®¡ç®— FLOPs
        print("ğŸ“Š è®¡ç®—æ¨¡å‹çš„ FLOPs...")
        flops = compute_flops(net, dummy_input)
        print(f"FLOPs: {flops:,}")

        # é‡æ–°å¼€å§‹è¿­ä»£ï¼ˆåŒ…æ‹¬ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
        dataloader_iter = iter(dataloader)

        for batch_idx in tqdm(range(len(dataloader)), desc="Running Inference"):
            try:
                batch = next(dataloader_iter)
            except StopIteration:
                break

            inputs = batch['inputs'].to(device)
            file_names = batch['file_name']

            # æµ‹é‡æ¨ç†æ—¶é—´
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()

            # æ¨¡å‹æ¨ç†
            logits = net(inputs)

            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()

            # ç´¯è®¡æ—¶é—´
            batch_time = end_time - start_time
            total_inference_time += batch_time
            total_samples += inputs.shape[0]

            # è°ƒæ•´ç»´åº¦é¡ºåº (B, C, H, W) -> (B, H, W, C)
            logits = logits.permute(0, 2, 3, 1)

            # è·å–é¢„æµ‹ç»“æœ
            pred = torch.argmax(logits, dim=-1).cpu().numpy()

            # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
            for i in range(inputs.shape[0]):
                # è·å–åŸå§‹æ–‡ä»¶å
                original_name = os.path.splitext(file_names[i])[0]

                # åˆ›å»ºé¢„æµ‹å›¾åƒ
                pred_i = pred[i]
                pred_img = Image.fromarray(pred_i.astype(np.uint8), mode='P')
                pred_img = apply_color_palette(pred_img)

                # ä¿å­˜é¢„æµ‹å›¾åƒ
                pred_img.save(os.path.join(save_dir, f"{original_name}.png"))

    # è®¡ç®—å¹¶æ‰“å°æ¨ç†é€Ÿåº¦ç»Ÿè®¡
    # ... åç»­ä»£ç ä¿æŒä¸å˜ ...

    print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='TSViT Inference Only')
    parser.add_argument('--config', required=True, help='Path to config YAML')
    parser.add_argument('--device', default='0', type=str, help='GPU device ids (comma-separated)')
    parser.add_argument('--weights', required=True, help='Path to trained weights (e.g., best.pth)')
    args = parser.parse_args()

    device_ids = [int(d) for d in args.device.split(',')]
    device = get_device(device_ids, allow_cpu=False)

    config = read_yaml(args.config)
    config['local_device_ids'] = device_ids

    dataloaders = get_dataloaders(config)

    # === åŠ¨æ€ Normalize ç»Ÿè®¡é˜¶æ®µ ===
    print("ğŸ“Š å¼€å§‹ç»Ÿè®¡è®­ç»ƒé›†å‡å€¼ä¸æ ‡å‡†å·®...")
    normalize_obj = None
    for t in dataloaders['train'].dataset.transform.transforms:
        if isinstance(t, Normalize):
            t.compute_stats = True
            normalize_obj = t
            break
    if normalize_obj is None:
        raise RuntimeError("æœªæ‰¾åˆ° Normalize å®ä¾‹ï¼Œè¯·æ£€æŸ¥ transforms é¡ºåºã€‚")
    with torch.no_grad():
        for sample in tqdm(dataloaders['train'], desc="Accumulating mean/std"):
            _ = sample
    normalize_obj.compute_mean_std()
    normalize_obj.compute_stats = False
    print("âœ… Normalize ç»Ÿè®¡å®Œæˆ")


    # === æ¨¡å‹é¢„æµ‹ ===
    net = get_model(config, device)

    # è®¡ç®—å¹¶æ‰“å°æ¨¡å‹å‚æ•°é‡
    print("=" * 50)
    print(f"ğŸ“Š æ¨¡å‹æ¶æ„: {config['MODEL']['architecture']}")

    # å¦‚æœæ˜¯ DataParallelï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if len(device_ids) > 1:
        # å¤šGPUæ¨¡å¼ä¸‹ï¼Œå…ˆåŒ…è£…æˆDataParallelå†è®¡ç®—
        net = nn.DataParallel(net, device_ids=device_ids)
        total_params = sum(p.numel() for p in net.module.parameters())
        trainable_params = sum(p.numel() for p in net.module.parameters() if p.requires_grad)
    else:
        total_params = sum(p.numel() for p in net.parameters())
        trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)

    print(f"ğŸ“ˆ æ€»å‚æ•°é‡: {total_params:,} ({total_params / 1e6:.2f}M)")
    print(f"âš™ï¸  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"ğŸ“‰ ä¸å¯è®­ç»ƒå‚æ•°é‡: {total_params - trainable_params:,}")
    print("=" * 50)

    load_from_checkpoint(net, args.weights, device)
    net.to(device)

    if len(device_ids) > 1:
        net = nn.DataParallel(net, device_ids=device_ids)

    # åˆ›å»ºæ›´æœ‰åºçš„ä¿å­˜è·¯å¾„
    config_name = os.path.splitext(os.path.basename(args.config))[0]
    test_save_dir = os.path.join(
        config['CHECKPOINT']['save_path'],
        "test_predictions",
        config_name
    )

    test_and_save_predictions(
        net,
        dataloaders['test'],
        config,
        device,
        save_dir=test_save_dir
    )

    print("âœ… æ¨ç†å®Œæˆï¼Œé¢„æµ‹ç»“æœå·²ä¿å­˜ã€‚")


#   python train_and_eval/predict_merge.py
#   --config configs/PASTIS24/TSViT_fold5.yaml --device 0
#   --weights  C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/PASTIS24/changji_kongjianshijian_TSViT_fold5/best.pth
#    --config configs/PASTIS24/UNET3D.yaml --device 0   --weights  C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/PASTIS24/changji_UNET3D/best.pth
#    --config configs/PASTIS24/UNET3Df.yaml   --device 0   --weights  C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/cahngji/UNET3Df/best.pth
#     --config configs/PASTIS24/UNet2D_CLSTM.yaml   --device 0   --weights C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/changji/changji_UNet2D_CLSTM/best.pth
#  --weights  C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/PASTIS24/changji_kongjianshijian_TSViT_fold5/best.pth
#    --config configs/PASTIS24/BiConvGRU.yaml --device 0     --weights    C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/changji/BiconvGRU/best.pth

#   --config configs/PASTIS24/TSViT_fold5.yaml --device 0    --weights    C:/Users/Think/Desktop/DeepSatModels-main/models/saved_models/changji/tsvit/best.pth

# C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/changji/unet3d/best.pth
# C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/PASTIS24_flod1/fold1_shijiankongjian/best.pth
#  C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/cahngji/UNET3Df/best.pth
# C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/changji/tsvit/best.pth
#  C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/changji/changji_UNet2D_CLSTM/best.pth
# C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/changji/BiconvGRU/best.pth

