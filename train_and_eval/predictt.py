import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import argparse
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from PIL import Image
import yaml
import time
import datetime
import json
from pathlib import Path

# === Model imports ===
from models.UNet3D.unet3d import UNet3D
from models.UNet3D.unet3df import UNet3D_CSCL
from models.CropTypeMapping.models import FCN_CRNN
from models.BiConvRNN.biconv_rnn import BiRNNSequentialEncoder
from models.TSViT.TSViTdense import TSViT
from data.PASTIS24.data_transforms import Normalize
from data import get_dataloaders
import torchprofile


def get_model(config, device):
    model_config = config['MODEL']
    if model_config['architecture'] == "UNET3Df":
        return UNet3D_CSCL(model_config).to(device)
    if model_config['architecture'] == "UNET3D":
        return UNet3D(model_config).to(device)
    if model_config['architecture'] == "UNET2D-CLSTM":
        return FCN_CRNN(model_config).cuda()
    if model_config['architecture'] == "ConvBiRNN":
        return BiRNNSequentialEncoder(model_config, device).to(device)
    if model_config['architecture'] == "TSViT":
        return TSViT(model_config).to(device)
    raise NameError(f"Model architecture '{model_config['architecture']}' not supported.")


def read_yaml(yaml_path):
    if not os.path.exists(yaml_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {yaml_path}")
    with open(yaml_path, 'r', encoding='utf-8') as f:
        try:
            config = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise RuntimeError(f"YAML è§£æå¤±è´¥: {e}")
    return config


def get_device(device_ids, allow_cpu=True):
    if torch.cuda.is_available():
        return torch.device(f"cuda:{device_ids[0]}")
    elif allow_cpu:
        return torch.device("cpu")
    else:
        raise EnvironmentError("æ²¡æœ‰å¯ç”¨ GPU ä¸”æœªå¯ç”¨ CPUã€‚")


def load_from_checkpoint(model, checkpoint_path, device):
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ¨¡å‹æƒé‡æœªæ‰¾åˆ°: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint)
    print(f"âœ… æˆåŠŸåŠ è½½æƒé‡: {checkpoint_path}")
    return checkpoint_path


def apply_color_palette(img):
    # 4 ç±»é¢œè‰²ï¼šé»‘ã€çº¢ã€ç»¿ã€è“
    palette = [
                  0, 0, 0,  # class 0 - black
                  255, 0, 0,  # class 1 - red
                  0, 255, 0,  # class 2 - green
                  0, 0, 255  # class 3 - blue
              ] + [0] * (256 * 3 - 12)  # å¡«å……å‰©ä½™
    img.putpalette(palette)
    return img


def compute_flops(model, input_tensor1, input_tensor2, seq_lengths):
    """
    è®¡ç®—æ¨¡å‹çš„FLOPs
    æ³¨æ„ï¼šç”±äºæ¨¡å‹éœ€è¦å¤šä¸ªè¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŒ…è£…å‡½æ•°
    """

    def forward_wrapper(x1, x2, lengths):
        return model(x1, x2, lengths)

    # ä½¿ç”¨torchprofileè®¡ç®—FLOPs
    flops = torchprofile.profile_macs(forward_wrapper, (input_tensor1, input_tensor2, seq_lengths))
    return flops


def save_model_stats_to_log(model_stats, log_dir, config_name):
    """
    ä¿å­˜æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    """
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"model_stats_{config_name}_{timestamp}.json"
    log_path = os.path.join(log_dir, log_filename)

    # ä¿å­˜ä¸ºJSONæ ¼å¼
    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(model_stats, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {log_path}")
    return log_path


def save_inference_stats_to_log(inference_stats, log_dir, config_name):
    """
    ä¿å­˜æ¨ç†ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    """
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"inference_stats_{config_name}_{timestamp}.json"
    log_path = os.path.join(log_dir, log_filename)

    # ä¿å­˜ä¸ºJSONæ ¼å¼
    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(inference_stats, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ æ¨ç†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {log_path}")
    return log_path


def test_and_save_predictions(net, dataloader, config, device, save_dir="test_predictions",
                              stats_log_dir=None, config_name=None):
    """
    æµ‹è¯•å¹¶ä¿å­˜é¢„æµ‹ç»“æœï¼ŒåŒæ—¶è®°å½•æ¨ç†æ€§èƒ½
    """
    os.makedirs(save_dir, exist_ok=True)
    net.eval()

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    model_stats = {}
    inference_stats = {}

    # æ·»åŠ æ¨ç†æ—¶é—´æµ‹é‡
    total_inference_time = 0.0
    total_samples = 0
    batch_times = []

    with torch.no_grad():
        # åŠ¨æ€è·å–è¾“å…¥å½¢çŠ¶
        print("ğŸ“ è·å–è¾“å…¥å½¢çŠ¶...")
        first_batch = next(iter(dataloader))

        # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆä¿æŒåŸæœ‰çš„ç»´åº¦å¤„ç†é€»è¾‘ï¼‰
        inputs = first_batch['inputs'].to(device)
        inputs_backward = first_batch['inputs_backward'].to(device)
        seq_lengths = first_batch['seq_lengths'].to(device)

        # è®­ç»ƒæ—¶çš„ç»´åº¦é¡ºåºè°ƒæ•´ï¼Œä¿æŒä¸è®­ç»ƒè¿‡ç¨‹ä¸€è‡´
        inputs_forward = inputs.permute(0, 4, 1, 2, 3).contiguous()  # [B, C, T, H, W]
        inputs_backward = inputs_backward.permute(0, 2, 1, 3, 4).contiguous()  # [B, C, T, H, W]

        input_shape_forward = inputs_forward.shape
        input_shape_backward = inputs_backward.shape

        model_stats['input_shape_forward'] = list(input_shape_forward)
        model_stats['input_shape_backward'] = list(input_shape_backward)

        # print(f"æ£€æµ‹åˆ°å‰å‘è¾“å…¥å½¢çŠ¶: {input_shape_forward}")
        # print(f"æ£€æµ‹åˆ°åå‘è¾“å…¥å½¢çŠ¶: {input_shape_backward}")

        # ä½¿ç”¨å®é™…è¾“å…¥å½¢çŠ¶è¿›è¡Œé¢„çƒ­
        print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
        dummy_forward = torch.randn_like(inputs_forward)
        dummy_backward = torch.randn_like(inputs_backward)
        dummy_lengths = torch.ones_like(seq_lengths) * seq_lengths.max()

        for _ in range(5):
            _ = net(dummy_forward, dummy_backward, dummy_lengths)

        # è®¡ç®— FLOPs
        print("ğŸ“Š è®¡ç®—æ¨¡å‹çš„ FLOPs...")
        flops = compute_flops(net, dummy_forward, dummy_backward, dummy_lengths)
        print(f"FLOPs: {flops:,}")

        # ä¿å­˜FLOPsåˆ°ç»Ÿè®¡ä¿¡æ¯
        model_stats['FLOPs'] = int(flops)
        model_stats['FLOPs_formatted'] = f"{flops:,}"
        model_stats['FLOPs_G'] = flops / 1e9

        # é‡æ–°å¼€å§‹è¿­ä»£ï¼ˆåŒ…æ‹¬ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
        dataloader_iter = iter(dataloader)

        for batch_idx in tqdm(range(len(dataloader)), desc="Running Inference"):
            try:
                batch = next(dataloader_iter)
            except StopIteration:
                break

            inputs = batch['inputs'].to(device)
            inputs_backward = batch['inputs_backward'].to(device)
            seq_lengths = batch['seq_lengths'].to(device)
            file_names = batch['file_name']

            # è®­ç»ƒæ—¶çš„ç»´åº¦é¡ºåºè°ƒæ•´ï¼Œä¿æŒä¸è®­ç»ƒè¿‡ç¨‹ä¸€è‡´
            inputs_forward = inputs.permute(0, 4, 1, 2, 3).contiguous()  # [B, C, T, H, W]
            inputs_backward = inputs_backward.permute(0, 2, 1, 3, 4).contiguous()  # [B, C, T, H, W]

            # æµ‹é‡æ¨ç†æ—¶é—´
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.perf_counter()

            # æ¨¡å‹æ¨ç†
            logits = net(inputs_forward, inputs_backward, seq_lengths)

            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.perf_counter()

            # ç´¯è®¡æ—¶é—´
            batch_time = end_time - start_time
            total_inference_time += batch_time
            batch_times.append(batch_time)
            total_samples += inputs.shape[0]

            # è°ƒæ•´ç»´åº¦é¡ºåº (B, C, H, W) -> (B, H, W, C)
            logits = logits.permute(0, 2, 3, 1)

            # è·å–é¢„æµ‹ç»“æœ
            pred = torch.argmax(logits, dim=-1).cpu().numpy()

            # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
            for i in range(inputs.shape[0]):
                # è·å–åŸå§‹æ–‡ä»¶å
                original_name = os.path.splitext(file_names[i])[0]

                # åˆ›å»ºé¢„æµ‹å›¾åƒ
                pred_i = pred[i]
                pred_img = Image.fromarray(pred_i.astype(np.uint8), mode='P')
                pred_img = apply_color_palette(pred_img)

                # ä¿å­˜é¢„æµ‹å›¾åƒ
                pred_img.save(os.path.join(save_dir, f"{original_name}.png"))

    # è®¡ç®—æ¨ç†æ€§èƒ½ç»Ÿè®¡
    avg_time_per_sample = total_inference_time / total_samples
    fps = total_samples / total_inference_time if total_inference_time > 0 else 0
    min_batch_time = min(batch_times) if batch_times else 0
    max_batch_time = max(batch_times) if batch_times else 0
    avg_batch_time = np.mean(batch_times) if batch_times else 0
    std_batch_time = np.std(batch_times) if batch_times else 0

    # ä¿å­˜æ¨ç†ç»Ÿè®¡ä¿¡æ¯
    inference_stats = {
        'total_inference_time_seconds': float(total_inference_time),
        'total_samples': int(total_samples),
        'average_time_per_sample_seconds': float(avg_time_per_sample),
        'inference_speed_fps': float(fps),
        'batch_time_statistics': {
            'min_batch_time_seconds': float(min_batch_time),
            'max_batch_time_seconds': float(max_batch_time),
            'average_batch_time_seconds': float(avg_batch_time),
            'std_batch_time_seconds': float(std_batch_time),
            'total_batches': len(batch_times)
        }
    }

    if torch.cuda.is_available():
        max_memory_mb = torch.cuda.max_memory_allocated(device) / 1024 ** 2
        inference_stats['gpu_memory_usage_mb'] = float(max_memory_mb)
        print(f"GPUå†…å­˜ä½¿ç”¨: {max_memory_mb:.2f} MB")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("=" * 50)
    print("ğŸ“ˆ æ¨ç†æ€§èƒ½ç»Ÿè®¡:")
    print(f"æ€»æ¨ç†æ—¶é—´: {total_inference_time:.2f}ç§’")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"å¹³å‡æ¯æ ·æœ¬æ¨ç†æ—¶é—´: {avg_time_per_sample:.4f}ç§’")
    print(f"æ¨ç†é€Ÿåº¦: {fps:.2f}æ ·æœ¬/ç§’")
    print(
        f"æ‰¹æ¬¡æ—¶é—´ç»Ÿè®¡: æœ€å°={min_batch_time:.4f}ç§’, æœ€å¤§={max_batch_time:.4f}ç§’, å¹³å‡={avg_batch_time:.4f}ç§’, æ ‡å‡†å·®={std_batch_time:.4f}ç§’")
    print("=" * 50)

    print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_dir}")

    # ä¿å­˜æ¨ç†ç»Ÿè®¡åˆ°æ—¥å¿—æ–‡ä»¶
    if stats_log_dir and config_name:
        save_inference_stats_to_log(inference_stats, stats_log_dir, config_name)

    return inference_stats, model_stats


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='TSViT Inference Only')
    parser.add_argument('--config', required=True, help='Path to config YAML')
    parser.add_argument('--device', default='0', type=str, help='GPU device ids (comma-separated)')
    parser.add_argument('--weights', required=True, help='Path to trained weights (e.g., best.pth)')
    args = parser.parse_args()

    device_ids = [int(d) for d in args.device.split(',')]
    device = get_device(device_ids, allow_cpu=False)

    config = read_yaml(args.config)
    config['local_device_ids'] = device_ids

    dataloaders = get_dataloaders(config)

    # === Normalize statistics phase ===
    print("ğŸ“Š å¼€å§‹ç»Ÿè®¡è®­ç»ƒé›†å‡å€¼ä¸æ ‡å‡†å·®...")
    normalize_obj = None
    for t in dataloaders['train'].dataset.transform.transforms:
        if isinstance(t, Normalize):
            t.compute_stats = True
            normalize_obj = t
            break
    if normalize_obj is None:
        raise RuntimeError("æœªæ‰¾åˆ° Normalize å®ä¾‹ï¼Œè¯·æ£€æŸ¥ transforms é¡ºåºã€‚")
    with torch.no_grad():
        for sample in tqdm(dataloaders['train'], desc="Accumulating mean/std"):
            _ = sample
    normalize_obj.compute_mean_std()
    normalize_obj.compute_stats = False
    print("âœ… Normalize ç»Ÿè®¡å®Œæˆ")

    # === Model prediction ===
    net = get_model(config, device)

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    print("=" * 50)
    print(f"ğŸ“Š æ¨¡å‹æ¶æ„: {config['MODEL']['architecture']}")

    # å¦‚æœæ˜¯ DataParallelï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if len(device_ids) > 1:
        net = nn.DataParallel(net, device_ids=device_ids)
        total_params = sum(p.numel() for p in net.module.parameters())
        trainable_params = sum(p.numel() for p in net.module.parameters() if p.requires_grad)
    else:
        total_params = sum(p.numel() for p in net.parameters())
        trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)

    # è®¡ç®—ä¸å¯è®­ç»ƒå‚æ•°
    non_trainable_params = total_params - trainable_params

    # ä¿å­˜æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
    config_name = os.path.splitext(os.path.basename(args.config))[0]
    model_stats = {
        'model_architecture': config['MODEL']['architecture'],
        'total_parameters': int(total_params),
        'total_parameters_formatted': f"{total_params:,}",
        'total_parameters_M': total_params / 1e6,
        'trainable_parameters': int(trainable_params),
        'trainable_parameters_formatted': f"{trainable_params:,}",
        'non_trainable_parameters': int(non_trainable_params),
        'non_trainable_parameters_formatted': f"{non_trainable_params:,}",
        'config_file': args.config,
        'weights_file': args.weights,
        'device_ids': device_ids,
        'device': str(device),
        'timestamp': datetime.datetime.now().isoformat()
    }

    # æ‰“å°å‚æ•°é‡ä¿¡æ¯
    print(f"ğŸ“ˆ æ€»å‚æ•°é‡: {total_params:,} ({total_params / 1e6:.2f}M)")
    print(f"âš™ï¸  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"ğŸ“‰ ä¸å¯è®­ç»ƒå‚æ•°é‡: {non_trainable_params:,}")
    print("=" * 50)

    # åŠ è½½æ¨¡å‹æƒé‡
    weights_path = load_from_checkpoint(net, args.weights, device)
    net.to(device)

    if len(device_ids) > 1:
        net = nn.DataParallel(net, device_ids=device_ids)

    # åˆ›å»ºä¿å­˜è·¯å¾„
    config_name = os.path.splitext(os.path.basename(args.config))[0]
    test_save_dir = os.path.join(
        config['CHECKPOINT']['save_path'],
        "test_predictions",
        config_name
    )

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    stats_log_dir = os.path.join(
        config['CHECKPOINT']['save_path'],
        "stats_logs",
        config_name
    )

    # è¿è¡Œæµ‹è¯•å¹¶è·å–ç»Ÿè®¡ä¿¡æ¯
    inference_stats, flops_stats = test_and_save_predictions(
        net,
        dataloaders['test'],
        config,
        device,
        save_dir=test_save_dir,
        stats_log_dir=stats_log_dir,
        config_name=config_name
    )

    # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
    all_stats = {
        **model_stats,
        **flops_stats,
        'inference_statistics': inference_stats
    }

    # ä¿å­˜å®Œæ•´ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    save_model_stats_to_log(all_stats, stats_log_dir, config_name)

    print("âœ… æ¨ç†å®Œæˆï¼Œé¢„æµ‹ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜ã€‚")

#   python train_and_eval/predictt.py
#   --config configs/PASTIS24/BiConvGRU.yaml --device 0
#   --weights     C:/Users/vipuser/Desktop/DeepSatModels-main/models/saved_models/changji/BiconvGRU/best.pth